spray.can.server {
  request-timeout = 3000s
  idle-timeout = 4000s
}

app {
  interface = "0.0.0.0"
  port = 80
}

mysql {
  #host = "10.3.12.10"                                        # MySQL 地址
  host = "10.19.11.230"
  port = 3306                                                # MySQL 端口
  db   = "ldadb"                                             # DB
  #user = "ldadev"                                            # DB user
  #password = "ldadev1234"
  user = "root"
  password = "111111"
}

analyzer {
  #baseURL = "http://10.3.12.2:8666/analyzer"
  baseURL = "http://10.19.11.230:8081/config/analyzer"                 # Stanford CoreNLP 所需的文件URL
}

#lda {
#  model.uri = "hdfs://10.3.12.9:9000/xxx/model"                                                          # LDA MODEL URI
#  stopword.path = "hdfs://10.3.12.9:9000/test/stopword.dic"                                              # 停用词路径
#  vocab.path = "hdfs://10.3.12.9:9000/xxx/vocab"                                                         # lda 训练的词汇ID路径
#  doc.topic.path = "hdfs://10.3.12.9:9000/xxx/predictHistoryOutput"                                          # 每个document的topic
#  train.docs.participated = "hdfs://10.3.12.9:9000/xxx/article"
#}
lda {
  model.uri = "hdfs://10.19.132.18:9000/xxx/model"                  # LDA MODEL URI，LocalLDAModel 的输出
  stopword.path = "hdfs://10.19.132.18:9000/test/stopword.dic"      # 停用词路径
  vocab.path = "hdfs://10.19.132.18:9000/users/root/lda/vocab2"     # lda 训练的词汇ID路径，LocalLDAModel 的输出
  doc.topic.path = "hdfs://10.19.132.18:9000/test/PrWord/Word2"     # 每个document的topic，PredictHistory 的输出
  train.docs.participated = "hdfs://10.19.132.18:9000/xxx/article"  # 分词后的文章,SegmentJob_1的输出
}


#predict {
#  model.uri = "hdfs://10.3.12.9:9000/xxx/model"                     # LDA 训练出来的Model存储路径
#  topic.result.path = "hdfs://10.3.12.9:9000/xxx/topicResult"       # LDA 训练出来的topic结果路径
#  vocab.path = "hdfs://10.3.12.9:9000/xxx/vocab"                    # LDA 训练过程中的vocab存储路径
#  history = "hdfs://10.3.12.9:9000/xxx/article"                     # Segment Job输出，分词后的文章
#  table = "article_20"                                              # 原始文章的数据库表名
#  topic5.word.table.name = "xxx_topics_5word"                       # 每个topic 5个词汇的mysql表名
#  topic10.word.table.name = "xxx_topics10"                          # 每个topic 10个词汇的mysql表名
#}
predict {
  model.uri = "hdfs://10.19.132.18:9000/xxx/model"                  # LDA 训练出来的Model存储路径
  topic.result.path = "hdfs://10.19.132.18:9000/xxx/topicResult"    # LDA 训练出来的topic结果路径
  vocab.path = "hdfs://10.19.132.18:9000/xxx/vocab"                 # LDA 训练过程中的vocab存储路径
  table = "article_20"                                               # 原始文章的数据库表名
  topic5.word.table.name = "xxx_topics_5word"                        # 每个topic 5个词汇的mysql表名
  topic10.word.table.name = "xxx_topics10"                           # 每个topic 10个词汇的mysql表名
}
